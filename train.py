import pandas as pd
import os
import os
import random
import numpy as np
import torch


def seed_everything(seed: int = 42, deterministic: bool = True):
    """
    Seed everything for reproducibility in PyTorch.

    Args:
        seed (int): Random seed
        deterministic (bool): If True, makes CUDA deterministic (slower)
    """
    # Python
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)

    # NumPy
    np.random.seed(seed)

    # PyTorch
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    if deterministic:
        # CuDNN
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

        # CUDA >= 10.2
        torch.use_deterministic_algorithms(True)

        # Needed for CUDA deterministic behavior
        os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"
    else:
        torch.backends.cudnn.benchmark = True
        
seed_everything(42, deterministic=True)
# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •
file_path = '/scratch2/tjgus0408/CMU/data/Labeled_OpenAP.csv'

# 2. ì‚¬ìš©í•  ì»¬ëŸ¼ ëª©ë¡ ì •ì˜
selected_columns = [
    'HexIdent', 
    'Date_MSG_Generated', 
    'Time_MSG_Generated',
    'Date_MSG_Logged', 
    'Time_MSG_Logged', 
    'Altitude', 
    'GroundSpeed',
    'Track', 
    'Latitude', 
    'Longitude', 
    'VerticalRate', 
    'IsOnGround',
    "Phase"
]

# 3. ë°ì´í„° ë¡œë“œ (íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ í›„ ë¡œë“œ)
if os.path.exists(file_path):
    try:
        # usecols íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì½ì–´ì˜¤ë©´ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        df = pd.read_csv(file_path, usecols=selected_columns)
        
        print("âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
        print(f"ë°ì´í„° í¬ê¸° (í–‰, ì—´): {df.shape}")
        
        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        print("\n[ìƒìœ„ 5ê°œ í–‰]")
        print(df.head())
        
        # ë°ì´í„° íƒ€ì… ë° ê²°ì¸¡ì¹˜ í™•ì¸
        print("\n[ë°ì´í„° ì •ë³´]")
        print(df.info())
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
else:
    print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
# 1. ë‚ ì§œì™€ ì‹œê°„ì„ í•©ì³ì„œ í•˜ë‚˜ì˜ datetime ì»¬ëŸ¼ ìƒì„± (Date_MSG_Generated + Time_MSG_Generated ì‚¬ìš©)
# ë°ì´í„°ê°€ ë¬¸ìì—´(object) í˜•íƒœì—¬ë„ pd.to_datetimeì´ ë˜‘ë˜‘í•˜ê²Œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.
print("â³ ë‚ ì§œì™€ ì‹œê°„ ë³‘í•© ì¤‘...")
df['Timestamp'] = pd.to_datetime(df['Date_MSG_Generated'] + ' ' + df['Time_MSG_Generated'])

df_multi = df.sort_values(by=['HexIdent', 'Timestamp']).set_index(['HexIdent', 'Timestamp'])

# 1. ê³ ìœ í•œ ê°’ ì¶”ì¶œ (NumPy ë°°ì—´ í˜•íƒœ)
# df_multi = df.sort_values(by=['HexIdent', 'Timestamp']).set_index(['HexIdent', 'Timestamp'])
unique_hex_ids = df["HexIdent"].unique()

# 2. (ì„ íƒ ì‚¬í•­) íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (ë‹¤ë£¨ê¸° ë” í¸í•¨)
hex_id_list = unique_hex_ids.tolist()

# ê²°ê³¼ í™•ì¸
print(f"ì´ ê³ ìœ  ID ê°œìˆ˜: {len(hex_id_list)}")
print("ID ëª©ë¡ (ìƒìœ„ 10ê°œ):", hex_id_list[:10])
# ---------------------------------------------------------
# [ê°€ì •] dfê°€ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆê³ , Timestamp ì»¬ëŸ¼ì´ datetime í˜•ì‹ì´ë¼ê³  ê°€ì •
# (ë§Œì•½ MultiIndexë¼ë©´ reset_index()ë¥¼ ë¨¼ì € ìˆ˜í–‰í•˜ì—¬ ì»¬ëŸ¼ìœ¼ë¡œ ë‚´ë ¤ì¤ë‹ˆë‹¤)
if 'HexIdent' not in df.columns or 'Timestamp' not in df.columns:
    df = df.reset_index()
# ---------------------------------------------------------

# 1. ë°ì´í„° ê¸¸ì´ê°€ 20ê°œ ì´í•˜ì¸ HexIdent ì œê±° (Filtering)
# ---------------------------------------------------------
print(f"ì „ì²˜ë¦¬ ì „ ë°ì´í„° í¬ê¸°: {len(df)}")

# ê° HexIdentë³„ ë°ì´í„° ê°œìˆ˜ ê³„ì‚°
counts = df.groupby('HexIdent').size()

# 20ê°œë³´ë‹¤ í° HexIdentë§Œ ì¶”ì¶œ (Index ë¦¬ìŠ¤íŠ¸)
valid_hex_ids = counts[counts > 20].index

# í•´ë‹¹ HexIdentë¥¼ ê°€ì§„ í–‰ë§Œ ë‚¨ê¹€
df_filtered = df[df['HexIdent'].isin(valid_hex_ids)].copy()

print(f"20ê°œ ì´í•˜ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {len(df_filtered)}")


# 2. 1ì‹œê°„ ì´ìƒ ê³µë°±ì´ ìˆìœ¼ë©´ ë¶„ë¦¬ (Segmentation)
# ---------------------------------------------------------

# (ì¤‘ìš”) HexIdentë³„, ê·¸ë¦¬ê³  ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ì´ ë˜ì–´ ìˆì–´ì•¼ ì •í™•íˆ ê³„ì‚°ë¨
df_filtered = df_filtered.sort_values(by=['HexIdent', 'Timestamp'])

# (1) ì´ì „ í–‰ê³¼ì˜ ì‹œê°„ ì°¨ì´ ê³„ì‚° (HexIdent ê·¸ë£¹ë³„ë¡œ ìˆ˜í–‰)
# HexIdentê°€ ë°”ë€ŒëŠ” ì§€ì ì€ diffê°€ ì´ìƒí•˜ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë‚˜, ì–´ì°¨í”¼ ê·¸ë£¹ë³„ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ìƒê´€ì—†ìŒ
df_filtered['time_diff'] = df_filtered.groupby('HexIdent')['Timestamp'].diff()

# (2) ì‹œê°„ ì°¨ì´ê°€ 1ì‹œê°„('1h')ë³´ë‹¤ í° ê²½ìš°ë¥¼ ì°¾ìŒ (True/False)
# ì²« ë²ˆì§¸ í–‰(NaT)ì€ ìƒˆë¡œìš´ ì‹œì‘ì´ë¯€ë¡œ False(0) ì²˜ë¦¬ í›„ ë¡œì§ ì ìš©
threshold = pd.Timedelta(minutes=30)
df_filtered['is_new_segment'] = (df_filtered['time_diff'] > threshold).fillna(False)

# (3) ëˆ„ì  í•©(cumsum)ì„ í†µí•´ ê·¸ë£¹ ë‚´ì—ì„œ Segment ë²ˆí˜¸ ë¶€ì—¬
# ì˜ˆ: [False, False, True, False] -> [0, 0, 1, 1]
df_filtered['segment_id'] = df_filtered.groupby('HexIdent')['is_new_segment'].cumsum()


# 3. ìµœì¢… Instance ID ìƒì„± (HexIdent + Segment ë²ˆí˜¸)
# ---------------------------------------------------------
# ì˜ˆ: A4CE9D_0, A4CE9D_1 ...
df_filtered['Unique_ID'] = df_filtered['HexIdent'] + "_" + df_filtered['segment_id'].astype(str)

# ë¶ˆí•„ìš”í•œ ì„ì‹œ ì»¬ëŸ¼ ì‚­ì œ
df_final = df_filtered.drop(columns=['time_diff', 'is_new_segment', 'segment_id'])

# ê²°ê³¼ í™•ì¸
print(f"ì´ ìƒì„±ëœ Instance(ë¹„í–‰ ë‹¨ìœ„) ê°œìˆ˜: {df_final['Unique_ID'].nunique()}")
print(df_final[['HexIdent', 'Timestamp', 'Unique_ID']].head(10))

# ---------------------------------------------------------
# 4. ì €ì¥ ë° í™œìš© (Dictionary í˜•íƒœë¡œ ë³€í™˜ ì¶”ì²œ)
# ---------------------------------------------------------

# ê° Unique_IDë¥¼ Keyë¡œ, í•´ë‹¹ ë°ì´í„°í”„ë ˆì„ì„ Valueë¡œ ê°–ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
instance_dict = {k: v for k, v in df_final.groupby('Unique_ID')}

# (ì„ íƒ ì‚¬í•­) ë¶„í•  í›„, ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ì€ InstanceëŠ” ë‹¤ì‹œ ì œê±°í•˜ê¸°
final_counts = df_final.groupby('Unique_ID').size()
valid_instances = final_counts[final_counts > 20].index
df_final_clean = df_final[df_final['Unique_ID'].isin(valid_instances)]

instance_dict = {k: v for k, v in df_final_clean.groupby('Unique_ID')}

import pandas as pd
import numpy as np

# ---------------------------------------------------------
# ìˆ˜ì •ëœ Rolling Majority Vote í•¨ìˆ˜ (ì¸ë±ìŠ¤ ì¶©ëŒ ë°©ì§€)
# ---------------------------------------------------------
def rolling_majority_vote(series, window):
    # 1. ì¸ë±ìŠ¤ ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ ê°’ë§Œ ì¶”ì¶œ (numpy array)
    values = series.values 
    
    # 2. ë²”ì£¼í˜• -> ìˆ«ìí˜• ë³€í™˜
    # pd.factorizeëŠ” (codes, uniques)ë¥¼ ë°˜í™˜í•˜ë©°, codesëŠ” numpy arrayì…ë‹ˆë‹¤.
    codes, uniques = pd.factorize(values)
    
    # factorizeì—ì„œ NaNì€ -1ì´ ë˜ë¯€ë¡œ, ì´ë¥¼ float NaNìœ¼ë¡œ ë³€í™˜ (Rolling ê³„ì‚°ìš©)
    codes_float = codes.astype(float)
    codes_float[codes == -1] = np.nan
    
    # 3. Rolling ìˆ˜í–‰ (Pandas Seriesë¡œ ì ì‹œ ë³€í™˜í•˜ì—¬ rolling ì‚¬ìš©)
    # ì´ë•Œ ì¸ë±ìŠ¤ëŠ” ë¬´ì‹œí•˜ê³  ê°’ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì•ˆì „í•©ë‹ˆë‹¤.
    rolled = pd.Series(codes_float).rolling(window=window, center=True, min_periods=1)
    
    # ìµœë¹ˆê°’ ê³„ì‚° í•¨ìˆ˜
    def get_window_mode(x):
        valid_votes = x[~np.isnan(x)]
        if len(valid_votes) == 0:
            return np.nan
        vals, counts = np.unique(valid_votes, return_counts=True)
        return vals[np.argmax(counts)]

    # apply ì ìš© (ê²°ê³¼ëŠ” numpy arrayë¡œ ë°›ìŒ)
    filled_codes = rolled.apply(get_window_mode, raw=True).values
    
    # 4. ìˆ«ì -> ì›ë˜ ë¬¸ìì—´(Phase) ë³µêµ¬
    # ì›ë³¸ ì‹œë¦¬ì¦ˆ ë³µì‚¬ (ì¸ë±ìŠ¤ ìœ ì§€ë¥¼ ìœ„í•´)
    filled_series = series.copy()
    
    # ì±„ì›Œì•¼ í•  ìœ„ì¹˜(mask) í™•ì¸
    mask = ~np.isnan(filled_codes)
    
    # [í•µì‹¬ ìˆ˜ì •] numpy array ë ˆë²¨ì—ì„œ ê°’ì„ í• ë‹¹í•˜ì—¬ ì¸ë±ìŠ¤ ì—ëŸ¬ ì›ì²œ ì°¨ë‹¨
    # filled_series.valuesë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ê°’ì„ ë„£ìŠµë‹ˆë‹¤.
    # uniques[int] ì¸ë±ì‹±ì„ ìœ„í•´ ë§ˆìŠ¤í¬ ëœ ë¶€ë¶„ë§Œ ì •ìˆ˜ë¡œ ë³€í™˜
    filled_series.values[mask] = uniques[filled_codes[mask].astype(int)]
    
    return filled_series

# ---------------------------------------------------------
# ì‹¤í–‰ ì½”ë“œ (ì´ì „ê³¼ ë™ì¼)
# ---------------------------------------------------------
print("â³ Local Majority Voting ì‹œì‘...")

WINDOW_SIZE = 15

for unique_id, df_instance in instance_dict.items():
    if df_instance['Phase'].isna().sum() > 0:
        
        # ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ
        voted_phase = rolling_majority_vote(df_instance['Phase'], WINDOW_SIZE)
        
        # ê²°ê³¼ ì—…ë°ì´íŠ¸ (fillna ì‚¬ìš©)
        df_instance['Phase'] = df_instance['Phase'].fillna(voted_phase)
        
        # Fallback (ì•ë’¤ ì±„ì›€)
        if df_instance['Phase'].isna().sum() > 0:
             df_instance['Phase'] = df_instance['Phase'].ffill().bfill()

print("âœ… ëª¨ë“  Instanceì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# í™•ì¸
total_nans = sum(df['Phase'].isna().sum() for df in instance_dict.values())
print(f"ë‚¨ì€ Phase ê²°ì¸¡ì¹˜ ê°œìˆ˜: {total_nans}")

import torch
from torch.utils.data import Dataset
import pandas as pd
import numpy as np
import os

class FlightTimeSeriesDataset(Dataset):
    def __init__(self, instance_dict, window_size=30, stride=1, phase_col='Phase', stat_path='./stats.npz'):
        """
        Args:
            instance_dict: {Unique_ID: DataFrame}
            stat_path: mean, std, phase_mapì„ ì €ì¥/ë¡œë“œí•  ê²½ë¡œ
        """
        self.window_size = window_size
        self.stride = stride
        self.instance_dict = instance_dict
        self.phase_col = phase_col
        self.stat_path = stat_path
        
        # ë©”íƒ€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì–´ë–¤ ì¸ìŠ¤í„´ìŠ¤ì˜ ëª‡ ë²ˆì§¸ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ì§€ ì €ì¥)
        self.samples = [] 
        
        # 1. Feature ì •ì˜ (TimeDeltaëŠ” ë‚´ë¶€ ê³„ì‚°)
        self.raw_features = [
            'Altitude', 'GroundSpeed', 'Track', 
            'Latitude', 'Longitude', 'VerticalRate'
        ]
        
        # ---------------------------------------------------------
        # Step 1: í†µê³„ê°’(Mean, Std) ë° Label Map ë¡œë“œ ë˜ëŠ” ê³„ì‚°
        # ---------------------------------------------------------
        if os.path.exists(stat_path):
            print(f"ğŸ“‚ ì €ì¥ëœ í†µê³„ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤: {stat_path}")
            loaded = np.load(stat_path, allow_pickle=True)
            self.mean = loaded['mean']
            self.std = loaded['std']
            self.phase_map = loaded['phase_map'].item()
            self.idx_to_phase = {v: k for k, v in self.phase_map.items()}
        else:
            print("ğŸ“Š í†µê³„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ê³„ì‚°í•©ë‹ˆë‹¤...")
            self._calculate_stats_and_map()
            
        print(f"âœ… Label Map: {self.phase_map}")

        # ---------------------------------------------------------
        # Step 2: Indexing (ì–´ë–¤ ë°ì´í„°ë¥¼ êº¼ë‚¼ì§€ ì£¼ì†Œë¡ ë§Œë“¤ê¸°)
        # ---------------------------------------------------------
        print("â³ ìƒ˜í”Œ ì¸ë±ì‹± ìƒì„± ì¤‘...")
        for unique_id, df in instance_dict.items():
            if len(df) < window_size:
                continue
                
            # Window ê°œìˆ˜ ê³„ì‚°
            num_samples = (len(df) - window_size) // stride + 1
            
            for i in range(num_samples):
                start_idx = i * stride
                # (DataFrame Key, Start Index) íŠœí”Œ ì €ì¥ -> ë©”ëª¨ë¦¬ ì ˆì•½
                self.samples.append((unique_id, start_idx))
                
        print(f"âœ… Dataset ì¤€ë¹„ ì™„ë£Œ! ì´ ìƒ˜í”Œ ìˆ˜: {len(self.samples)}")

    def _calculate_stats_and_map(self):
        """ì „ì²´ ë°ì´í„°ë¥¼ ìˆœíšŒí•˜ë©° Mean, Std ê³„ì‚° ë° Label Map ìƒì„±"""
        all_data_list = []
        all_phases = set()
        
        for df in self.instance_dict.values():
            # (1) Time Delta
            dt = df['Timestamp'].diff().dt.total_seconds().fillna(0).values.reshape(-1, 1)
            # (2) Features
            others = df[self.raw_features].interpolate().fillna(0).values
            
            combined = np.hstack([dt, others])
            all_data_list.append(combined)
            
            # Label ìˆ˜ì§‘
            unique_phases = df[self.phase_col].dropna().unique()
            all_phases.update(unique_phases)
            
        # í†µê³„ ê³„ì‚°
        full_data = np.vstack(all_data_list)
        self.mean = np.mean(full_data, axis=0)
        self.std = np.std(full_data, axis=0)
        self.std[self.std == 0] = 1.0 # 0 ë‚˜ëˆ„ê¸° ë°©ì§€
        
        # Label Map ìƒì„±
        sorted_phases = sorted(list(all_phases))
        self.phase_map = {phase: idx for idx, phase in enumerate(sorted_phases)}
        self.idx_to_phase = {idx: phase for phase, idx in self.phase_map.items()}
        
        # ì €ì¥
        np.savez(self.stat_path, mean=self.mean, std=self.std, phase_map=self.phase_map)
        print(f"ğŸ’¾ í†µê³„ ë° ë§µì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {self.stat_path}")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        """ì—¬ê¸°ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìë¥´ê³  ì •ê·œí™”í•¨"""
        unique_id, start_idx = self.samples[idx]
        df = self.instance_dict[unique_id]
        
        end_idx = start_idx + self.window_size
        
        # 1. ë°ì´í„° ìŠ¬ë¼ì´ì‹± (Pandas iloc ì‚¬ìš©)
        # í•„ìš”í•œ ë¶€ë¶„ë§Œ ì˜ë¼ëƒ„
        subset = df.iloc[start_idx : end_idx]
        
        # 2. Feature ì¶”ì¶œ
        # ì£¼ì˜: Time DeltaëŠ” ì „ì²´ df ê¸°ì¤€ì´ ì•„ë‹ˆë¼, ì˜ë¦° ìœˆë„ìš° ë‚´ì—ì„œì˜ ì°¨ì´ê°€ ì•„ë‹˜.
        # ì›ë³¸ ë°ì´í„°ì˜ ì—°ì†ì„±ì„ ìœ ì§€í•´ì•¼ í•˜ë¯€ë¡œ, ë¯¸ë¦¬ ê³„ì‚°ëœ ê°’ì„ ì“°ê±°ë‚˜
        # ì—¬ê¸°ì„œ ê³„ì‚°í•˜ë˜ ì²« ë²ˆì§¸ ê°’ ì²˜ë¦¬ì— ìœ ì˜í•´ì•¼ í•¨.
        # ê°€ì¥ ì•ˆì „í•œ ë°©ë²•: ì „ì²´ DFì—ì„œ dtë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•´ë‘ëŠ” ê²ƒì´ ì¢‹ì§€ë§Œ, 
        # ì—¬ê¸°ì„œëŠ” ë¶€ë¶„ ê³„ì‚° ë¡œì§ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ ìœˆë„ìš° ë‚´ ì²« ê°’ì€ 0ì´ ë  ìˆ˜ ìˆìŒì„ ê°ì•ˆ.
        
        # (ë” íš¨ìœ¨ì ì¸ ë°©ì‹: __init__ì—ì„œ ê° DFì— 'dt' ì»¬ëŸ¼ì„ ë¯¸ë¦¬ ì¶”ê°€í•´ë‘ëŠ” ê²ƒ ì¶”ì²œ)
        # ì—¬ê¸°ì„œëŠ” ì›ë³¸ ë¡œì§ ìœ ì§€:
        dt = subset['Timestamp'].diff().dt.total_seconds().fillna(0).values.reshape(-1, 1)
        others = subset[self.raw_features].interpolate().fillna(0).values # ë¶€ë¶„ ë³´ê°„
        
        raw_x = np.hstack([dt, others])
        
        # 3. [í•µì‹¬] ì •ê·œí™” (ì‹¤ì‹œê°„ ìˆ˜í–‰)
        normalized_x = (raw_x - self.mean) / self.std
        
        # 4. Label ê°€ì ¸ì˜¤ê¸° (ë§ˆì§€ë§‰ ì‹œì )
        last_phase_str = subset[self.phase_col].iloc[-1]
        y_label = self.phase_map.get(last_phase_str, -1) # ì—†ìœ¼ë©´ -1
        
        return torch.FloatTensor(normalized_x), torch.LongTensor([int(y_label)])

    def decode_label(self, idx):
        return self.idx_to_phase.get(idx, "Unknown")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from sklearn.metrics import classification_report # ë¼ë²¨ë³„ ì •í™•ë„ ê³„ì‚°ìš© (sklearn ì•ˆì“´ë‹¤ê³  í•˜ì…¨ì§€ë§Œ metrics ê³„ì‚°ì—” ìœ ìš©. ì•ˆì“°ë ¤ë©´ ì§ì ‘ ê³„ì‚° ì½”ë“œ ëŒ€ì²´ ê°€ëŠ¥)
import torch
import torch.nn as nn
import torch.optim as optim

class FlightPhaseLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2):
        super(FlightPhaseLSTM, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM Layer
        # batch_first=True: ì…ë ¥ì´ (Batch, Seq, Feature) ìˆœì„œ
        self.lstm = nn.LSTM(
            input_size=input_size, 
            hidden_size=hidden_size, 
            num_layers=num_layers, 
            batch_first=True, 
            dropout=dropout if num_layers > 1 else 0
        )
        
        # Classifier (Fully Connected)
        self.fc = nn.Linear(hidden_size, num_classes)
        
    def forward(self, x):
        # x shape: (Batch, Window_Size, Input_Size)
        
        # ì´ˆê¸° Hidden Stateì™€ Cell State (0ìœ¼ë¡œ ì´ˆê¸°í™”)
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # LSTM ìˆœì „íŒŒ
        # out shape: (Batch, Window_Size, Hidden_Size)
        out, _ = self.lstm(x, (h0, c0))
        
        # Many-to-One: ë§ˆì§€ë§‰ Time Stepì˜ ê²°ê³¼ë§Œ ì‚¬ìš©
        last_out = out[:, -1, :] 
        
        # ë¶„ë¥˜
        logits = self.fc(last_out)
        return logits

# ì„¤ì •
WINDOW_SIZE = 20
STRIDE = 15
BATCH_SIZE = 512

# 1. Dataset ìƒì„±
# (instance_dictëŠ” ì´ì „ì— ë§Œë“  ê²ƒì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
dataset = FlightTimeSeriesDataset(instance_dict, window_size=WINDOW_SIZE, stride=STRIDE)

# 2. DataLoader ìƒì„±
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# 3. í™•ì¸
data_iter = iter(dataloader)
features, labels = next(data_iter)

print("\n[ê²€ì¦ ê²°ê³¼]")
print(f"Input Features Shape: {features.shape}") # (Batch, 30, 7)
print(f"Labels Shape: {labels.shape}")           # (Batch, 1)

# ë°ì´í„°ê°€ ì •ê·œí™”ê°€ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸ (0 ê·¼ì²˜ì˜ ê°’ì´ì–´ì•¼ í•¨)
print("\n[Sample Data - First Timestep Features]")
print("ìˆœì„œ: [DeltaTime, Alt, GSpd, Trk, Lat, Lon, VRate]")
print(features[0][0].numpy()) # ì²« ë°°ì¹˜, ì²« ìœˆë„ìš°, ì²« íƒ€ì„ìŠ¤í…
# ---------------------------------------------------------
# 1. Dataset Split (Train: 80%, Val: 20%)
# ---------------------------------------------------------

BATCH_SIZE = 2048

# ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸
dataset_size = len(dataset)
train_size = int(dataset_size * 0.8)
val_size = dataset_size - train_size

# random_splitì„ ì´ìš©í•´ ë¬´ì‘ìœ„ë¡œ ë¶„í• 
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

print(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {dataset_size}")
print(f"ğŸ”¹ í•™ìŠµìš©(Train): {len(train_dataset)}")
print(f"ğŸ”¸ ê²€ì¦ìš©(Val):   {len(val_dataset)}")

# DataLoader ìƒì„± (Valì€ shuffle=Falseê°€ ì¼ë°˜ì )
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)


# ---------------------------------------------------------
# ì„¤ì • ë° ì´ˆê¸°í™”
# ---------------------------------------------------------
INPUT_SIZE = 7
HIDDEN_SIZE = 64
NUM_LAYERS = 2
NUM_CLASSES = len(dataset.phase_map)
LEARNING_RATE = 0.001
EPOCHS = 20
LOG_INTERVAL = 50  # [ì¶”ê°€] 50ë²ˆì§¸ ë°°ì¹˜ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥ (ë°ì´í„° í¬ê¸°ì— ë§ì¶° ì¡°ì ˆí•˜ì„¸ìš”)

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = FlightPhaseLSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, NUM_CLASSES).to(DEVICE)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Best Model ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜
best_val_loss = float('inf')
save_path = 'best_flight_phase_lstm_20history.pth'

print(f"ğŸš€ í•™ìŠµ ë° ê²€ì¦ ì‹œì‘... (Log Interval: {LOG_INTERVAL})")

# ---------------------------------------------------------
# í•™ìŠµ ë£¨í”„
# ---------------------------------------------------------
for epoch in range(EPOCHS):
    # ==========================
    # 1. Training Phase
    # ==========================
    model.train()
    train_loss = 0
    train_correct = 0
    train_total = 0
    
    # ë°°ì¹˜ ê°œìˆ˜ íŒŒì•…
    total_batches = len(train_loader)
    
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        inputs = inputs.to(DEVICE)
        targets = targets.squeeze().to(DEVICE)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        train_total += targets.size(0)
        train_correct += (predicted == targets).sum().item()

        # [ì¶”ê°€] ì¤‘ê°„ ë¡œê·¸ ì¶œë ¥
        if (batch_idx + 1) % LOG_INTERVAL == 0:
            current_loss = loss.item()
            current_acc = 100 * (predicted == targets).sum().item() / targets.size(0)
            print(f"   [Epoch {epoch+1}/{EPOCHS}] Batch {batch_idx+1}/{total_batches} | Loss: {current_loss:.4f} | Acc: {current_acc:.2f}%")

    avg_train_loss = train_loss / len(train_loader)
    train_acc = 100 * train_correct / train_total

    # ==========================
    # 2. Validation Phase
    # ==========================
    model.eval()
    val_loss = 0
    val_correct = 0
    val_total = 0
    
    all_targets = []
    all_preds = []
    
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs = inputs.to(DEVICE)
            targets = targets.squeeze().to(DEVICE)
            
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            
            val_total += targets.size(0)
            val_correct += (predicted == targets).sum().item()
            
            all_targets.extend(targets.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())

    avg_val_loss = val_loss / len(val_loader)
    val_acc = 100 * val_correct / val_total

    # ==========================
    # 3. ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
    # ==========================
    print(f"\nâœ… Epoch [{epoch+1}/{EPOCHS}] ì™„ë£Œ")
    print(f"   Train | Loss: {avg_train_loss:.4f} | Acc: {train_acc:.2f}%")
    print(f"   Val   | Loss: {avg_val_loss:.4f}   | Acc: {val_acc:.2f}%")
    
    if avg_val_loss < best_val_loss:
        print(f"   ğŸ’¾ Validation Loss ê°œì„ ! ({best_val_loss:.4f} -> {avg_val_loss:.4f}) ëª¨ë¸ ì €ì¥ ì¤‘...")
        best_val_loss = avg_val_loss
        torch.save(model.state_dict(), save_path)
    
    # ==========================
    # 4. ë¼ë²¨ë³„ ì •í™•ë„ (Class-wise Accuracy)
    # ==========================
    print("   [Label-wise Accuracy]")
    
    class_correct = {i: 0 for i in range(NUM_CLASSES)}
    class_total = {i: 0 for i in range(NUM_CLASSES)}
    
    for t, p in zip(all_targets, all_preds):
        class_total[t] += 1
        if t == p:
            class_correct[t] += 1
            
    for idx in range(NUM_CLASSES):
        label_name = dataset.decode_label(idx)
        count = class_total[idx]
        if count > 0:
            acc = 100 * class_correct[idx] / count
            print(f"     - {label_name:<5}: {acc:.1f}% ({class_correct[idx]}/{count})")
        else:
            print(f"     - {label_name:<5}: N/A")
    
    print("-" * 60) # êµ¬ë¶„ì„ 

print("\nğŸ‰ ëª¨ë“  í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ğŸ† ìµœê³ ì˜ ëª¨ë¸ì€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")